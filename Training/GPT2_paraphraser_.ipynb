{"cells":[{"cell_type":"markdown","source":["### Init libraries"],"metadata":{"id":"RzmkuV0VHuV3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Rb1BSLugcZn"},"outputs":[],"source":["!pip install transformers\n","!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltEwIIMDg9ex"},"outputs":[],"source":["import logging\n","import math\n","import os\n","from dataclasses import dataclass, field\n","from typing import Optional\n","import pickle\n","\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    AutoConfig,\n","    GPT2LMHeadModel,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    LineByLineTextDataset,\n","    PreTrainedTokenizer,\n","    TextDataset,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n",")\n","import random\n","import wandb\n","import pickle\n","import random"]},{"cell_type":"markdown","metadata":{"id":"FCfdGqMWVvMR"},"source":["# 2022 importing and formatting clean data from GYAFC:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hz8eXpRYX806"},"outputs":[],"source":["!gdown --id \n","!gdown --id \n","!gdown --id \n","!gdown --id \n","\n","\n","with open('/content/3_all_acl_cleaned.pkl', 'rb') as handle:\n","    all_acl_cleaned = pickle.load(handle)"]},{"cell_type":"code","source":["bos = \"<BOS>\"\n","eos = \"<EOS>\"\n","sg = \"<SCI_GEN>\""],"metadata":{"id":"qV25PN7dR8Uh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GStB5rPYVPD"},"outputs":[],"source":["#sents_gyafc = [bos + row[\"formal\"] + sg + row[\"informal\"] + eos for index, row in cleaned_gyafc.iterrows()]\n","#sents_train_gyafc = [bos + row[\"formal\"] + sg + row[\"informal\"] + eos for index, row in cleaned_train_gyafc.iterrows()]\n","#sents_paranmt = [bos + row[\"t1\"] + sg + row[\"t2\"] + eos for index, row in cleaned_paranmt.iterrows()]\n","sents_acl = [bos + row[\"output\"] + sg + row[\"input\"] + eos for index, row in all_acl_cleaned.iterrows()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FM00S1_NLvL"},"outputs":[],"source":["# Save data\n","#sents_gyafc.extend(sents_paranmt)\n","random.shuffle(sents_acl)\n","\n","textfile = open(\"/content/train.txt\", \"w\")\n","for element in sents_acl[1000:]:\n","    textfile.write(element + \"\\n\")\n","textfile.close()\n","\n","textfile = open(\"/content/eval.txt\", \"w\")\n","for element in sents_acl[:1000]:\n","    textfile.write(element + \"\\n\")\n","textfile.close()"]},{"cell_type":"markdown","metadata":{"id":"0-3v4LjSVzig"},"source":["# Old code continued:"]},{"cell_type":"markdown","source":["### Model arguments"],"metadata":{"id":"ezgxCJTmG6Ed"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpg78swJV2lz"},"outputs":[],"source":["\n","# Setup logging\n","logger = logging.getLogger(__name__)\n","\n","# Get access to model types and model configs to select GPT2 model and config\n","MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IL0YR9WMhG0v"},"outputs":[],"source":["@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n","    \"\"\"\n","\n","    model_name_or_path: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n","        },\n","    )\n","    model_type: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"If training from scratch, pass a model type from the list: \"\n","            + \", \".join(MODEL_TYPES)\n","        },\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"Where do you want to store the pretrained models downloaded from s3\"\n","        },\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fX9aWWf1hL4X"},"outputs":[],"source":["@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","    \"\"\"\n","\n","    train_data_file: Optional[str] = field(\n","        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n","    )\n","    eval_data_file: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"\n","        },\n","    )\n","    line_by_line: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"\n","        },\n","    )\n","\n","    mlm: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Train with masked-language modeling loss instead of language modeling.\"\n","        },\n","    )\n","\n","    block_size: int = field(\n","        default=-1,\n","        metadata={\n","            \"help\": \"Optional input sequence length after tokenization.\"\n","            \"The training dataset will be truncated in block of this size for training.\"\n","            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n","        },\n","    )\n","    \n","    overwrite_cache: bool = field(\n","        default=False,\n","        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n","    )\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Od_Gxsz1hZvV"},"outputs":[],"source":["# Create LineByLineDataset from Movie Plots text file\n","def get_dataset(\n","    args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False\n","):\n","    file_path = args.eval_data_file if evaluate else args.train_data_file\n","    if args.line_by_line:\n","        return LineByLineTextDataset(\n","            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size\n","        )\n","    else:\n","        return TextDataset(\n","            tokenizer=tokenizer,\n","            file_path=file_path,\n","            block_size=args.block_size,\n","            overwrite_cache=args.overwrite_cache,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeEGwWDf0SS1"},"outputs":[],"source":["customTokenList = [\"<SCI_GEN>\"]"]},{"cell_type":"markdown","source":["### Wandb"],"metadata":{"id":"4CM-GB84G8q9"}},{"cell_type":"code","source":["!wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ziKd0ds_Uf5E","executionInfo":{"status":"ok","timestamp":1642284881970,"user_tz":-60,"elapsed":1335,"user":{"displayName":"Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02305615210147278999"}},"outputId":"5268f941-7d38-4b5d-c6eb-49ed75e34abe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel_ml\u001b[0m (use `wandb login --relogin` to force relogin)\n"]}]},{"cell_type":"markdown","source":["### Run training"],"metadata":{"id":"o9vcGehDG_VK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXE6Zq8-hikF"},"outputs":[],"source":["def main():\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"gpt2-medium\", model_type=\"gpt2-medium\"\n","    )\n","    data_args = DataTrainingArguments(\n","        train_data_file=\"/content/train.txt\",\n","        eval_data_file=\"/content/eval.txt\",\n","        line_by_line=True,\n","        block_size=512,\n","        overwrite_cache=True,\n","    )\n","    training_args = TrainingArguments(\n","        output_dir=\"content/\",\n","        overwrite_output_dir=True,\n","        do_train=True,\n","        do_eval=True,\n","        logging_steps=500,\n","        per_device_train_batch_size=8,\n","        num_train_epochs=8,\n","        save_total_limit=8,\n","        save_strategy = \"epoch\",\n","    )\n","\n","    # Cut 1 was here\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        training_args.local_rank,\n","        training_args.device,\n","        training_args.n_gpu,\n","        bool(training_args.local_rank != -1),\n","        training_args.fp16,\n","    )\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed for deterministic training runs\n","    set_seed(training_args.seed)\n","\n","\n","    config = AutoConfig.from_pretrained(\n","        \"gpt2-medium\", cache_dir=model_args.cache_dir\n","    )\n","   \n","    tokenizer = AutoTokenizer.from_pretrained(\n","        \"gpt2-medium\", cache_dir=model_args.cache_dir\n","    )\n","\n","    model = GPT2LMHeadModel.from_pretrained(\n","        \"gpt2-medium\",\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","    )\n","    wandb.watch(model, log='all')\n","\n","    special_tokens_dict = {\n","        \"bos_token\": \"<BOS>\",\n","        \"eos_token\": \"<EOS>\",\n","        \"pad_token\": \"<PAD>\",\n","        \"additional_special_tokens\": customTokenList,\n","    }\n","\n","\n","    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n","    model.resize_token_embeddings(len(tokenizer))\n","    # Update the models understanding of the bos and eos tokens\n","    model.config.bos_token_id = tokenizer.bos_token_id\n","    model.config.eos_token_id = tokenizer.eos_token_id\n","    ###########################################################\n","\n","    # Cut 2 was here\n","\n","    train_dataset = (\n","        get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n","    )\n","    print('train_dataset: \\n' + str(len(train_dataset)))\n","    eval_dataset = (\n","        get_dataset(data_args, tokenizer=tokenizer, evaluate=True)\n","        if training_args.do_eval\n","        else None\n","    )\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=data_args.mlm,\n","    )\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        data_collator=data_collator,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","    )\n","\n","    # Training\n","    try:\n","      if training_args.do_train:\n","          model_path = (\n","              model_args.model_name_or_path\n","              if model_args.model_name_or_path is not None\n","              and os.path.isdir(model_args.model_name_or_path)\n","              else None\n","          )\n","          trainer.train(model_path=model_path)\n","          trainer.save_model()\n","          tokenizer.save_pretrained(training_args.output_dir)\n","    except KeyboardInterrupt:\n","      print(\"Saving model that was in the middle of training\")\n","      trainer.save_model()\n","      tokenizer.save_pretrained(training_args.output_dir)\n","      return\n","\n","    # Evaluation\n","    results = {}\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","\n","        eval_output = trainer.evaluate()\n","\n","        perplexity = math.exp(eval_output[\"eval_loss\"])\n","        result = {\"perplexity\": perplexity}\n","\n","        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n","        if trainer.is_world_process_zero():\n","            with open(output_eval_file, \"w\") as writer:\n","                logger.info(\"***** Eval results *****\")\n","                for key in sorted(result.keys()):\n","                    logger.info(\"  %s = %s\", key, str(result[key]))\n","                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","        results.update(result)\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdzdlR4ciaZ6"},"outputs":[],"source":["# Press the Run Cell button to the left to start training\n","if __name__ == \"__main__\":\n","  main()\n","\n","# To stop training and save model, press the same Run Cell button (now, it is the Interrupt Execution button)"]},{"cell_type":"markdown","source":["### Inference"],"metadata":{"id":"i-U2V_0LAjWy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQZfJnlVl9X1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642305862576,"user_tz":-60,"elapsed":9879,"user":{"displayName":"Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02305615210147278999"}},"outputId":"26eda0ab-513e-411c-8826-ec88ca1f9592"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/content/checkpoint-29474/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2-medium\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50257,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50258,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1024,\n","  \"n_head\": 16,\n","  \"n_inner\": null,\n","  \"n_layer\": 24,\n","  \"n_positions\": 1024,\n","  \"n_special\": 0,\n","  \"predict_special_tokens\": true,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.15.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50261\n","}\n","\n","loading weights file /content/content/checkpoint-29474/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/content/checkpoint-29474.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"]}],"source":["model = GPT2LMHeadModel.from_pretrained(\"/content/content/checkpoint-29474\")"]},{"cell_type":"code","source":["import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","_ = model.to(device)"],"metadata":{"id":"jkEeWITnpbfo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_string = \"We trained this model for the task of NER.\"\n","\n","encoded_sent = tokenizer.encode(\"<BOS>\"+ input_string + \"<SCI_GEN>\", return_tensors = \"pt\", return_attention_mask= True).to(device)\n","output = model.generate(inputs = encoded_sent , pad_token_id= tokenizer.pad_token_id, max_length = 200)\n","\n","tokenizer.decode(output[0], skip_special_tokens= False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"49HmTT9PpIGA","executionInfo":{"status":"ok","timestamp":1642305338397,"user_tz":-60,"elapsed":306,"user":{"displayName":"Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02305615210147278999"}},"outputId":"0762cc15-0483-4df6-da99-3c689775ff15"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<BOS>We trained this model for the task of NER.<SCI_GEN>we trained this model on the ner task<EOS>'"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-T7WtlbMCMGM","executionInfo":{"status":"ok","timestamp":1642305501623,"user_tz":-60,"elapsed":17632,"user":{"displayName":"Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02305615210147278999"}},"outputId":"8c17deb1-b3b9-494a-cee6-312f2cfe572e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["model.save_pretrained(\"/content/drive/MyDrive/MRP2_model_checkpoints/GPT2_model2_no_rewards/gpt2_model/epoch2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtEbRlXGFVq0","executionInfo":{"status":"ok","timestamp":1642305886356,"user_tz":-60,"elapsed":6296,"user":{"displayName":"Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02305615210147278999"}},"outputId":"6b976e27-c5ed-431b-83f9-769090e10a71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Configuration saved in /content/drive/MyDrive/MRP2_model_checkpoints/GPT2_model2_no_rewards/gpt2_model/epoch2/config.json\n","Model weights saved in /content/drive/MyDrive/MRP2_model_checkpoints/GPT2_model2_no_rewards/gpt2_model/epoch2/pytorch_model.bin\n"]}]},{"cell_type":"markdown","metadata":{"id":"v-4u64sXTnRA"},"source":["# Questions:\n",">Tags what do they do?\n",">Capitalisation not an issue for token understanding of model?\n",">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQ4GPsSfQrDH"},"outputs":[],"source":["# This cell is to style the Google Colab's output properly (Just blindly run this)\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUoTHvmHHz43"},"outputs":[],"source":["# Run these cells for story generation\n","from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n","\"\"\" \n","Below, my model checkpoint is commented out. You can replace your checkpoint \n","with that to test if your checkpoint didn't train for long enough\n","\"\"\"\n","checkpoint = \"/content/gdrive/MyDrive/startup/data/checkpoint-150000/\"\n","tokenizer_path = \"/content/gdrive/MyDrive/startup/data/\"\n","\n","model = GPT2LMHeadModel.from_pretrained(checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","story_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clXP7bNrPxja"},"outputs":[],"source":["input_prompt = \"\"\"\n","\"\"\"\n","\n","input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n","\n","# num_beams=5,\n","    # early_stopping=True    \n","\n","conv_output = model.generate(\n","    input_ids,\n","    max_length=100,\n","    top_p=0.90,\n","    temperature=0.90,\n","    top_k=50,\n","    do_sample=True,\n","    early_stopping=True\n",")\n","\n","# story_output = story_generator(input_prompt, \n","#                         max_length=100, \n","#                         do_sample=True,\n","#                         repetition_penalty=1.1, \n","#                         temperature=0.90,\n","#                         num_beams=5,\n","#                         early_stopping=True,\n","#                         top_p=0.90,\n","#                         top_k=50)\n","\n","# for conv in conv_output:\n","print(tokenizer.decode(conv_output[0], skip_special_tokens=True))\n","# print('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_B0tUE03DFq3"},"outputs":[],"source":["print(len(conv_output))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmT5bqkoW2fR"},"outputs":[],"source":["def score(tokens_tensor):\n","    loss = story_generator(tokens_tensor, labels=tokens_tensor)[0]\n","    return np.exp(loss.gpu().detach().numpy())\n","\n","\n","# for text in input:\n","tokens_tensor = tokenizer.encode(input, add_special_tokens=False, return_tensors=\"pt\")\n","print (input, score(tokens_tensor))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":" GPT2_paraphraser_.ipynb","provenance":[{"file_id":"1DyOBiENQny3IDJ88vmz747-qi9TF4OaQ","timestamp":1642674422438},{"file_id":"1aY49J68Ch1gaCvi09EENzYC8hEp55Nh7","timestamp":1642152162449},{"file_id":"16LO3UCk-Xycu9ZX6SOKkQ4mZGlidGRlI","timestamp":1641572242074},{"file_id":"1xPLPps1W59y3XvmNs02rTUbiPnjWKY-h","timestamp":1641481375078},{"file_id":"1oVgzmKiBZT15TngpNmeMYVmTkXb3TUkM","timestamp":1624008471183}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}